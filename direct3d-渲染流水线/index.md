# 渲染流水线


# ***渲染流水线***

## ***3D 视觉即错觉？***

*我们都知道物体重叠（object overlap）的概念，即不透明物体能够遮挡住其后侧物体的局部（或整 体），如图 5.4 所示。这是一个重要的概念，它传达了不同物体在场景中的深度顺序关系。而我们在第 4 章 中也已经讨论过如何在 Direct3D 中借助深度缓冲区来确定那些应当受到遮蔽而不是绘制出来的像素。*

*光照和阴影的处理在刻画 3D 物体的实体形状和立体感中扮演着至关重要的角色。*

## ***模型的表示***

*实际上，实体 3D 对象是借助三角形网格（triangle mesh）来近似表示的，因而我们要以三角形作为 3D 物体建模的基石，通常来讲，模拟一个物体所用的三角形越多，那么模型就与目标物体越接近，这是因为模型会随之获得更为丰富的细节。当然，建模所用的三角形越多，也就需要更强大的计算处理能 力，所以要根据应用受众的硬件性能做出权衡。*

*除了三角形，点和线也有其用武之地，若要手动列出这些三角形来模拟 3D 物体，实在是一件太麻烦的事儿了。除了最简单的模型， 我们可以使用一种叫作 3D 建模工具（3D modeler）的专用软件，来生成和处理复杂的 3D 对象，流行的建模软件有 3D Studio Max、LightWave 3D、 Maya、Softimage|XSI①和 Blender。其中，Blender 是开源和免费爱好者的福音*

## ***计算机色彩基础***

*计算机显示器中的每个像素发出的都是红、绿、蓝三色混合光*

*每款显示器所能发出的红、绿、蓝三色光的强度都是有限的。为了便于描述光的强度，我们常将它 量化为范围在 0～1 归一化区间中的值。0 代表无强度，1 则表示强度最大，处于两者之间的值就表示对 应的中间强度*

*例如，强度值（0.25, 0.67, 1.0）就表明此光线由强度为 25%的红色光、强度为 67%的绿色光以及强度为 100%的蓝色光混合而成*

***由此例可以看出，我们能用 3D 向量（r, g, b）来表示颜色，其中 0  ≤ r, g, b ≤ 1 ，这 3 种颜色分量分别代表红、绿、蓝三色光在混合光中的强度***

#### ***颜色运算***

*向量的部分运算规则在颜色向量上同样适用。例如，我们可以使两个颜色向量相加来得到新的颜色：*

*(0.0, 0.5, 0) + (0, 0.0, 0.25) = (0.0, 0.5, 0.25)*

*这就是说，通过混合中等强度的绿色和低等强度的蓝色，便会得到深绿色。*

*也可以通过颜色向量之间的减法运算来获得新的颜色*

*(1, 1, 1) - (1, 1, 0) = (0, 0, 1)*

*由上式可以看出，从白色中去掉红色和绿色的成分，便可得到蓝色。*

*标量乘法也是有效的，请考虑下式:*

*0.5(1, 1, 1) = (0.5, 0.5, 0.5)*

*此式将白色的各颜色分量取半，继而得到中等强度的灰色。另外，可通过 2(0.25, 0, 0) = (0.5, 0, 0)运算将 红色分量的强度加倍。*

*显而易见的是，像点积和叉积这样的运算法则就不适用于颜色向量了*

*颜色向量也有它们自己专属的颜色运算，即分量式（modulation 或 componentwise）乘法*
$$
(c_r, c_g, c_b) \otimes (k_r, k_g, k_b) = (c_rk_r, c_gk_g, c_bk_b)
$$
***这种运算主要应用于光照方程***

*例如，假设有颜色为（r, g, b）的入射光线，照射到一个反射 50%红色光、 75%绿色光、25%蓝色光且吸收剩余光的表面。那么，我们就可以据此给出反射光线的颜色: *
$$
(r, g, b) \otimes (0.5, 0.75, 0.25) = (0.5r, 0.75g, 0.25b)
$$
*通过此式即可看出，由于此表面会吸收一部分入射光，所以当它照射在该平面上时会损失掉部分颜色强度*

*在进行颜色运算的过程中，颜色分量有可能会超出 [ 0, 1 ] 这个区间。*

*如，思考 (1, 0.1, 0.6) + (0, 0.3, 0.5) = (1, 0.4, 1.1) 这个等式。*

*由于 1.0 代表颜色分量的最大强度，所以任何光的强度都不能超过此值。因此， 我们就只得将值为 1.1 的强度与 1.0 这一上限强度视作等同，将 1.1 钳制（clamp）为 1.0。同样地，显示 器也不能发出强度为负值的光，所以亦应把负的颜色分量（由减法运算所得到的结果）钳制为 0.0*

#### ***128 位颜色***

*事实上，我们通常还会用到另一种名为 alpha 分量（alpha component）的颜色分量。alpha 分量常用于表示颜色的不透明度 （opacity。值为 0.0 表示完全透明，值为 1.0 表示不透明），它在混合（blending）技术中起到了至关重要的作用。*

*这就是说， 把 alpha 分量算在内的话，我们就可以用 4D 向量(r, g, b, a)来表示每一种颜色，分量需要满足0  ≤ r, g, b, a≤ 1  。 为了用 128 位（128bit）数据来表示一种颜色，每个分量都要使用浮点值*

*由于每种颜色刚好能用数学上的 4D 向量来表示，所以我们也就能在代码中用 XMVECTOR 类型来描述它们*

#### ***32位颜色***

*为了用32 位（32bit）数据表示一种颜色，每个分量仅能分配到 1 个字节。*

*因此，每个占用 8 位字节的颜色 分量就可以分别描述256 种不同的颜色强度——0 代表无强度，255 是最大强度，处于两者之间的值也就表示相 应的中间强度。*

*每种颜色分量占用空间虽然看起来很小，但是它们的全部组合（256 × 256 × 256 = 16 777 216 ） 却能表示出千万种不同的颜色*

*DirectXMath 库（#include ) 在 DirectX::PackedVector 命名空间中提供了下面的结构用于存储 32 位颜色*

## ***渲染流水线概述***

*若给出某个 3D 场景的几何描述，并在其中架设一台具有确定位置和朝向的虚拟摄像机，那么渲染 流水线（rendering pipeline）是以此摄像机为观察视角而生成 2D 图像的一系列完整步骤。*

*图5.11 左侧展 示的是组成渲染流水线的所有阶段，而右侧则是显存资源*

*从资源内存池指向渲染流水线阶段的箭头， 表示该阶段可以访问资源并以此作为输入*

![流水线渲染](https://raw.githubusercontent.com/CuteCocoa/MyImage/main/流水线渲染.png)

## ***输入装配器阶段***

*输入装配器（Input Assembler，IA）阶段会从显存中读取几何数据（顶点和索引，vertex and index），再将它们装配为几何图元（geometric primitive，亦译作几何基元，如三角形和线条这种构成图形的基本元素）。*

*简单来说，我们是通过索引来定义如何将顶点装配在一起，从而构成图元的方法*

### ***顶点***

*在数学中，三角形的顶点是两条边的交点；线段的顶点是它的两个端点；而对于单个的点来说，它本身就是一个顶点*

*顶点除了空间位置以外, Direct3D 中的顶点还可以包含其他 信息，这使我们能够利用它来表现出更为复杂的渲染效果*

*例如，我们可以为顶点添加法向量，以实现光照效果。也可以为顶点添加纹理坐标，从而实现纹理贴图。*

*Direct3D 为用户自定义顶点格式提供了很高的灵活性（即它允许我们定义顶点结构体中的分量）*

### ***图元拓扑***

*Direct3D 中，我们要通过一种名为顶点缓冲区（vertex buffer）的特殊数据结构，将顶点与渲染流水线相绑定*

*顶点缓冲区利用连续的内存来存储一系列顶点。可是，仅凭这一点并不能说明这些顶点究竟如何组成几何图元*

*例如，我们应将顶点缓冲区内的顶点两两一组解释成线段，还是每 3 个一组解释 为三角形呢？*

*对此，我们要通过指定图元拓扑（primitive topology，或称基元拓扑）来告知 Direct3D 如 何用顶点数据来表示几何图元*

#### ***点列表***

*通过枚举项 D3D_PRIMITIVE_TOPOLOGY_POINTLIST 来指定点列表（point list）。当使用点列表 拓扑时，所有的顶点都将在绘制调用的过程中被绘制为一个单独的点*

#### ***线条带***

*通过枚举项 D3D_PRIMITIVE_TOPOLOGY_LINESTRIP 来指定线条带（line strip）。在使用线条带 拓扑时，顶点将在绘制调用的过程中被连接为一系列的连续线段（如图 5.13b 所示）。所以，在这种拓扑 模式下，若有 n+1 个顶点就会生成 n 条线段*

#### ***线列表***

*通过枚举项 D3D_PRIMITIVE_TOPOLOGY_LINELIST 来指定线列表（line list）。当使用线列表拓扑 时，每对顶点在绘制调用的过程中都会组成单独的线段（如图 5.13c 所示）。所以 2n 个顶点就会生成 n 条线段。线列表与线条带的区别是：线列表中的线段可以彼此分开，而线条带中的线段则是相连的。如 果线段相连的话，绘制同样数量的线段便会占用更少的顶点，因为每个处于线条带中间位置的顶点都可以同时被两条线段所共用*

#### ***三角形带***

*通过枚举项 D3D_PRIMITIVE_TOPOLOGY_TRIANGLESTRIP 来指定三角形带（triangle strip）。当使用 三角形带拓扑时，所绘制的三角形将像图 5.13d 所示的那样被连接成带状。可以看到，在这种三角形连接的 结构中，处于中间位置的顶点将被相邻的三角形所共同使用。因此，利用 n 个顶点即可生成 n−2 个三角形*

![image-20250211201354990](https://raw.githubusercontent.com/CuteCocoa/MyImage/main/image-20250211201354990.png)

#### ***三角形列表***

*通过枚举项 D3D_PRIMITIVE_TOPOLOGY_TRIANGLELIST 来指定三角形列表（triangle list）。当使用三角 形列表拓扑时，在绘制调用的过程中会将每 3 个顶点装配成独立的三角形（如图 5.14a 所示）；所以每 3n 个顶点会生成 n 个三角形。三角形列表与三角形带的区别是：三角形列表中的三角形可以彼此分离，而 三角形带中的三角形则是相连的*

#### ***具有邻接数据的图元拓扑***

*对于存有邻接数据的三角形列表而言，每个三角形都有 3 个与之相邻的邻接三角形（adjacent triangle）。图 5.14b 中展示的就是这种图元拓扑。在几何着色器中，往往需要访问这些邻接三角形来实现 特定的几何着色算法。为了使几何着色器可以顺利地获得这些邻接三角形的信息，我们就需要借助顶点 缓冲区与索引缓冲区（index buffer）将它们随主三角形一并提交至渲染流水线。另外，此时一定要将拓 扑类型指定为 D3D_PRIMITIVE_TOPOLOGY_TRIANGLELIST_ADJ，只有这样，渲染流水线才能得知如何以顶点缓冲区中的顶点来构建主三角形及其邻接三角形。注意，邻接图元的顶点只能用作几何着色器的输入数据，却并不会被绘制出来。即便程序没有用到几何着色器，但依旧不会绘制邻接图元*

![i图元拓扑2](https://raw.githubusercontent.com/CuteCocoa/MyImage/main/图元拓扑2.png)

#### ***控制点面片列表***

*D3D_PRIMITIVE_TOPOLOGY_N_CONTROL_POINT_PATCHLIST 拓扑类型表示：将顶点数据解释 为具有 N 个控制点（control point）的面片列表（patch list）。此图元常用于渲染流水线的曲面细分阶段 （tessellation stage，此环节为可选阶段）*

### ***索引***

*如前所述，三角形是 3D 实体对象的基本组成部分*

*为三角形指定顶点顺序是一项十分重要的工作，我们称这个顺序为绕序（winding order）*

*构成 3D 物体的不同三角形会共用许多顶点, 像八 边形那样的例子可就麻烦了. 因为每个三角形不仅都复制了一份中心顶点 v0，而且此八边 形边上的每个顶点都被两个三角形所同时共用。*

*一般来说，随着模型细节和复杂度的增加，复制顶点的数量亦会急剧上升。*

*我们不希望复制顶点数据的原因有两个：*

1. *增加内存的需求   —  为什么要多次存储同一个顶点数据呢?*
2. *增加图形硬件的处理负荷  —  为什么要多次处理同一个顶点数据呢？*

*借助三角形带可以在某些情况下改善顶点的复制问题，前提是这些几何体能够被组织为带状结构。 但是，由于三角形列表更为灵活（该拓扑中的三角形都无需互相连接），所以值得花些心思研究一种利用三角形列表移除重复顶点的设计方案*

*在此，我们所采用的解决方法是使用索引（index）。*

*整个工作流程是这样的：先创建一个顶点列表和一个索引列表。在顶点列表中收录一份所有独立的顶点，并在索引列表中存储顶点列表的索引值，这些索引定义了顶点列表中的顶点是如何组合在一起，从而构成三角形的*

*待处理完顶点列表中那些独立的顶点之后，显卡就能通过索引列表把顶点组合成一系列三角形。*

*可以看到，我们已经将“复用的顶点数据”转化为索引列表，但是这样做的效果要比之前的方法更好，这是因为：*

1. *索引皆是简单的整数，不会像使用整个顶点结构体那样占用更多的内存（而且，随着顶点结构体中分量的不断增多，将会使内存的需求变得更为急迫）。*
2. *若辅以适当的顶点缓存排序，则图形硬件将不必再次处理重复使用的顶点，从缓存中直接取得即可（这种情况十分普遍）*

## ***顶点着色器阶段***

*待图元被装配完毕后，其顶点就会被送入顶点着色器阶段（vertex shader stage，简记作 VS）。我们可以把顶点着色器看作一种输入与输出数据皆为单个顶点的函数。每个要被绘制的顶点都须经过顶点着色器的处理再送往后续阶段*

*其中的顶点着色器函数（VertexShader）就是我们要实现的那一部分，因为在这一阶段中对顶点 的操作实际是由 GPU 来执行的，所以速度很快*

*我们可以利用顶点着色器来实现许多特效，例如变换、光照和位移贴图（displacement mapping，也译作置换贴图。map 有映射之意，因此也有译作位移映射，类似的还有在后面将见到的纹理贴图、法线 贴图等）。请牢记：在顶点着色器中，不但可以访问输入的顶点数据，也能够访问纹理和其他存于显存中 的数据（如变换矩阵与场景的光照信息）。*

### ***局部空间和世界空间***

*局部坐标系通常是一种以目标物体的中心为原点（也有例外，视具体情况 而定），并且坐标轴与该物体对齐的简易便用坐标系*

*根据物体的位置与朝向，指定其局部空间坐标系的原点和诸坐标轴相对于全局场景坐标系的坐标，再运用坐标变换即可将物体从局部空间变换至世界空间*

*将局部坐标系内的坐标转换到全局场景坐标系中的过程叫作**世界变换（world transform）**，所使用的变换矩阵名为**世界矩阵（world matrix）**。*

*由于场景中每个物体的朝向和位置都可能各不相同，因此它们都有自己特定的世界 矩阵。当每个物体都从各自的局部空间变换到世界空间后，它们的坐标都将位于同一坐标系（即世界坐标系）之中。如果希望直接在世界空间内定义一个物体，那么就可以使用单位世界矩阵（identity world matrix）。*

*如果 $ Q_w = (Q_x, Q_y, Q_z, 1)， u_w = (u_x, u_y, u_z, 0)，v_w = (v_x, v_y, v_z, 0)和 w_w = (w_x, w_y, w_z, 0) $ 分别 描述了局部空间内的原点、x 轴、y 轴和 z 轴相对于世界空间的齐次坐标，那么从局部空间至世界空间的坐标变换矩阵为：*
$$
W = 
\begin{bmatrix}
u_x & u_y & u_z & 0
\\\ \\\
v_x & v_y & v_z & 0
\\\ \\\
w_x & w_y & w_z & 0
\\\ \\\
Q_x & Q_y & Q_z & 1
\end{bmatrix}
$$
*可以看到，为了构建一个世界矩阵，我们必须弄清局部空间中原点和各坐标轴相对于世界空间的坐标关系。但这样做有时并不简单亦不直观。*

***一种更常用的办法是定义一系列的变换组合 W，即 W = SRT。***

*首先，缩放矩阵 S 将物体缩放到世界空间；其次，旋转矩阵 R 用来定义局部空间相对于世界空间的朝向，最后，平移矩阵 T 定义的是局部空间的原点相对于世界空间的位置。*

*我们能够将这一系列变换视为一种坐标变换，而矩阵 W = SRT 中的行向量则分别存储的是局部空间的 x 轴、y 轴、z 轴及原点相对于全局空间的的齐次坐标。*

#### ***示例***

*假设我们在局部空间定义了一个单位正方形，其最小点和最大点的坐标分别为(−0.5, 0, −0.5)与(0.5, 0, 0.5)。*

*现在要求出一个世界矩阵，使此正方形在世界空间中的边长为 2，在世界空间 xz 平面内顺时针旋转 45° ，且中心位于世界空间的坐标(10, 0, 10)处。据此，我们构造矩阵 S、R、T 及 W 的过程如下：*
$$
S = 
\begin{bmatrix}
2 & 0 & 0 & 0
\\\ \\\
0 & 1 & 0 & 0
\\\ \\\
0 & 0 & 2 & 0
\\\ \\\
0 & 0 & 0 & 1
\end{bmatrix}
\~~~~~~~~~
R = 
\begin{bmatrix}
\frac{\sqrt{2}}{2} & 0 & -\frac{\sqrt{2}}{2} & 0
\\\ \\\
0 & 1 & 0 & 0
\\\ \\\
\frac{\sqrt{2}}{2} & 0 & \frac{\sqrt{2}}{2} & 0
\\\ \\\
0 & 0 & 0 & 1
\end{bmatrix}
\~~~~~~~~~
T = 
\begin{bmatrix}
1 & 0 & 0 & 0
\\\ \\\
0 & 1 & 0 & 0
\\\ \\\
0 & 0 & 1 & 0
\\\ \\\
10 & 0 & 10 & 1
\end{bmatrix}
\\\ \\\
W = SRT = 
\begin{bmatrix}
\sqrt{2} & 0 & -\sqrt{2} & 0
\\\ \\\
0 & 1 & 0 & 0
\\\ \\\
\sqrt{2} & 0 & \sqrt{2} & 0
\\\ \\\
10 & 0 & 10 & 1
\end{bmatrix}
$$
*矩阵 W 中的行向量描述了此正方形局部坐标系的诸坐标轴与原点相对于世界空间的坐标*

*即有  $ u_w = (\sqrt{2}, 0, -\sqrt{2}, 0)，v_w = (0, 1, 0, 0)， w_w = (\sqrt{2}, 0, \sqrt{2}, 0), Q_w = (10, 0, 10, 1) $ , 如果我们利用矩阵 W 将此局部空间向世界空间进行坐标变换，则最终会把正方形置于题设中所期望的世界空间内的预定位置*

1. *$ [-0.5, 0, -0.5, 1]W = [10 - \sqrt{2}, 0, 0, 1] $*
2. *$ [-0.5, 0, +0.5, 1]W = [0, 0, 10 + \sqrt{2}, 1] $*
3. *$ [+0.5, 0, +0.5, 1]W = [10 + \sqrt{2}, 0, 0, 1] $*、
4. *$  [+0.5, 0, -0.5, 1]W = [0, 0, 10 - \sqrt{2}, 1]  $*

*此例的亮点是在不指明 $ Q_W、u_W、v_W和 w_W $ 的情况下，直接通过复合一系列简单的变换来建立世界矩阵。这通常比用 $ Q_W、u_W、v_W和 w_W $求取世界矩阵的方法要容易得多，因为我们只需了解物体在世界空间中的大小、朝向以及位置即可*

*另一种考虑世界变换的观点是把局部空间坐标当作世界 空间坐标来看待（此方法就相当于用单位矩阵进行世界变 换）。这样一来，如果在物体局部空间的原点处建模，那么该 物体也就位于世界空间的原点处。通常来说，我们不大可能 把物体全都建立在世界空间的原点处。所以往往还是要为每个 物体运用一系列变换，使之缩放、旋转，并令其位于世界空间 中的预定位置。从数学角度上来讲，这种变换与由局部空间转 换至世界空间所用的坐标变换矩阵进行的是同一种世界变换*

### ***观察空间***

*为了构建场景的 2D 图像，我们必须在场景中架设一台虚拟摄像机。该摄像机确定了观察者可见的视野， 也就是生成 2D 图像所需的场景空间范围。对此，我们先为该摄像机赋予一个图 5.19 所示的局部坐标系（这被称作观察空间（view space），也译作观察坐标系、视图空间、视觉空间（eye space）或摄像机空间（camera space））*

![image-20250419212717546](https://raw.githubusercontent.com/vlicecream/cloudImage/main/image-20250419212717546.png)

*在此坐标系中，该虚拟摄像机位于原点 并沿 z 轴的正方向观察，x 轴指向摄像机的右侧， y 轴则指向摄像机的上方。与相对于世界空间来描述场景中的物体顶点不同，观察空间用于在渲染流水线的后续阶段中描述这些顶点相对于摄像机坐标系的坐标。由世界空间至观察空间的坐标变换称为取景变换（view transform，也译作观察变换、视图变换等），此变换所用的矩阵则称为观察 矩阵（view matrix，亦译作视图矩阵）。*

*如果 $ Q_w = (Q_x, Q_y, Q_z, 1)， u_w = (u_x, u_y, u_z, 0)，v_w = (v_x, v_y, v_z, 0)和 w_w = (w_x, w_y, w_z, 0) $ 分别 描述了局部空间内的原点、x 轴、y 轴和 z 轴相对于世界空间的齐次坐标，那么从局部空间至世界空间的坐标变换矩阵为：*
$$
W = 
\begin{bmatrix}
u_x & u_y & u_z & 0
\\\ \\\
v_x & v_y & v_z & 0
\\\ \\\
w_x & w_y & w_z & 0
\\\ \\\
Q_x & Q_y & Q_z & 1
\end{bmatrix}
$$
*然而这并不是我们所期待的变换。刚好相反，我们需要的是从世界空间到观察空间的这一逆变换。 逆变换可由变换矩阵的逆来求得，所以世界空间到观察空间的坐标变换矩阵为 $ W^{−1}。$*

*世界坐标系和观察坐标系通常只有位置和朝向这两点差异，所以由观察空间到世界空间的变换可以直接表示为 W = RT（即世界矩阵可以分解为一个旋转矩阵与一个平移矩阵的乘积）。此形式使得上述逆变换更易于计算：*
$$
V = W^{-1} = (RT^{-1}) = T^{-1}R^{-1} = T^{-1}R^T = 
\\\ \\\
\begin{bmatrix}
1 & 0 & 0 & 0
\\\ \\\
0 & 1 & 0 & 0
\\\ \\\
0 & 0 & 1 & 0
\\\ \\\
-Q_x & -Q_y & -Q_z, & 1
\end{bmatrix}
\begin{bmatrix}
u_x & v_x & w_x & 0
\\\ \\\
u_y & v_y & w_y & 0
\\\ \\\
u_z & v_z & w_z & 0
\\\ \\\
0 & 0 & 0 & 1
\end{bmatrix}
\=
\begin{bmatrix}
u_x & v_x & w_x & 0
\\\ \\\
u_y & v_y & w_y & 0
\\\ \\\
u_z & v_z & w_z & 0
\\\ \\\
-Q \cdot \vec{u} & -Q \cdot \vec{v}  & -Q \cdot \vec{w}, & 1
\end{bmatrix}
$$
![image-20250422202424820](https://raw.githubusercontent.com/vlicecream/cloudImage/main/image-20250422202424820.png)

*现在我们来展示一种用以构建观察矩阵中诸向量的直观方法。设 Q 为虚拟摄像机的位置，T 为此摄像机对准的观察目标点（target point）*

*接下来，设 j 为表示世界空间“向上”方向的单位向量。（我们用世界空间中的平面 xo 作为场景中的“地平面”，并以世界空间的 y 轴来指示场景内“向上”的方向。 因此，j = (0, 1, 0)仅是平行于世界空间中 y 轴的一个单位向量。有时为了方便起见，一些应用程序也可能选择平面 xy 作为地平面，而选 z 轴来指示“向上”的方向）对于图 5.20 来讲，虚拟摄像机的观察方向为：*
$$
\vec{w} = \frac{T - Q}{|| T - Q ||}
$$
*该向量表示虚拟摄像机局部空间的 z 轴。指向 w“右侧”的单位向量为：*
$$
\vec{u} = \frac{\vec{j} \times \vec{w}}{|| \vec{j} \times \vec{w} ||}
$$
*它表示的是虚拟摄像机局部空间的 x 轴。最后，该摄像机局部空间的 y 轴为：*
$$
\vec{v} = \vec{w} \times \vec{u}
$$
*因为 w 和 u 为互相正交的单位向量，所以 w x u 亦必为单位向量。由此，我们也就无须对向量 v 进 行规范化处理了。*

*综上所述，只要给定摄像机的位置、观察目标点以及世界空间中“向上”方向的向量，我们就能构建出对应的摄像机局部坐标系，并推导出相应的观察矩阵。*

*DirectXMath 库针对上述计算观察矩阵的处理流程提供了以下函数：*

```cpp
MMATRIX XM_CALLCONV XMMatrixLookAtLH( // 输出观察矩阵 V
 FXMVECTOR EyePosition, // 输入虚拟摄像机位置 Q
 FXMVECTOR FocusPosition, // 输入观察目标点 T
 FXMVECTOR UpDirection); // 输入世界空间中向上方向的向量
```

*一般来说，世界空间中的 y 轴方向与虚拟摄像机“向上”向量的方向相同，所以，我们通常将“向上”向量定为 j = (0, 1, 0)。举个例子，假设我们希望把虚拟摄像机架设于世界空间内点(5, 3, −10)的位置， 并令它观察世界空间的原点（0, 0, 0），则构建相应观察矩阵的过程为：*

```cpp
XMVECTOR pos = XMVectorSet(5, 3, -10, 1.0f);
XMVECTOR target = XMVectorZero();
XMVECTOR up = XMVectorSet(0.0f, 1.0f, 0.0f, 0.0f);
XMMATRIX V = XMMatrixLookAtLH(pos, target, up); 
```

### ***投影和齐次裁剪空间***

*摄像机在世界空间中的位置和朝向，除此之外，它还有另一个关键组成要素：即摄像机可观察到的空间体积（volume of space）。此范围可用一个由四棱锥截取的平截头体（frustum，即 四棱台）来表示（如图 5.21 所示）。*

![image-20250422203553333](https://raw.githubusercontent.com/vlicecream/cloudImage/main/image-20250422203553333.png)

*我们将平截头体内的 3D 几何体投 影到一个 2D 投影窗口（projection window）之中。 根据前文所述的透视投影（perspective projection） 的原理可知，投影必将沿众平行线汇聚于消失点 上，而且随着物体 3D 深度的增加，其投影的尺寸也将逐渐变小*

*我们将由顶点到观察 点（eye point，也译作视点）的连线称为顶点的投影线（vertex’s line of projection）。继而就可以定义出：将 3D 顶点 v 变换至其投影线与 2D 投影平面交点 v'的透视投影变换（perspective projection transformation）。我们称点 v'为点 v 的投影。3D 物体的投影即为构成该物体上所有顶点的投影。*

#### ***定义平截头体***

*在观察空间中，我们可以通过近平面（near plane，也译作近裁剪面）n、远平面（far plane，也译作远裁 剪面）f、垂直视场角（vertical field of view angle）α 以及纵横比（aspect ratio，也作宽高比）r 这 4 个参数来 定义一个：以原点作为投影的中心，并沿 z 轴正方向进行观察的平截头体（可参见图 5.23）。*

![image-20250422204932536](https://raw.githubusercontent.com/vlicecream/cloudImage/main/image-20250422204932536.png)

*值得注意的是， 位于观察空间中的远、近平面皆平行于平面 xy，因此，我们就能方便地确定出它们分别沿 z 轴到原点的距离。 纵横比的定义为 r = w/h，其中 w 为投影窗口的宽度，h 为投影窗口的高度（以观察空间的单位为准）。*

*我们通常将投影窗口的纵横比指定为后台缓冲区的纵横比 （比值并没有单位）如若投影窗口与后台缓冲区的纵横比不一致，那么映射的过程中，就需要对投影窗口在将投影窗口进行不等比缩放，继而导致图像出现拉伸变形的现象。*

1. *我们现在通过垂直视场角 α 和纵横比 r 来确定水平视场角 β 。注意，投影窗口的实际大小并不重要，关键在于确定纵横比。因此，出于方便， 我们将高定为 2，而宽则必满足：*
   $$
   r = \frac{w}{h} = \frac{w}{2} \Rightarrow w = 2r
   $$

2. *为了求出具体的垂直视场角α ，我们假定投影窗口到原点的距离为 d：*
   $$
   \tan({\frac{\alpha}{2}}) = \frac{1}{d} \Rightarrow d = \cot{\frac{\alpha}{2}}
   $$

3. *因此，当投影窗口的高度为 2 且垂直视场角为 α 时，我们就能确定该投影窗口沿 z 轴到观察点的距离 d。已知这些条件，即可求取水平视场角 β 。观察图 5.23 中的平面 xz 可以发现：*
   $$
   \tan{\frac{\beta}{2}} = \frac{r}{d} = \frac{r}{\cot(\frac{\alpha}{2})} = r \cdot \tan({\frac{\alpha}{2}})
   $$

4. *所以，一旦给定垂直视场角α 和纵横比 r，我们必能求出水平视场角 β ：*
   $$
   \beta = 2arctan(r \cdot \tan(\frac{\alpha}{2}))
   $$

5. 

#### ***投影顶点***

![image-20250422211832271](https://raw.githubusercontent.com/vlicecream/cloudImage/main/image-20250422211832271.png)

*我们希望求出给定点（x, y, z）在投影平面 z = d 中的投影（x', y', d），见图 5.24。通过在 x 轴和 y 轴 上分别利用相似三角形的性质，我们可以发现：*
$$
\frac{x'}{d} = \frac{x}{z} \Rightarrow x' = \frac{xd}{z} = \frac{x\cot(\alpha / 2)}{z} = \frac{x}{z\tan(\alpha) / 2}
\\\ \\\
\frac{y'}{d} = \frac{y}{z} \Rightarrow y' = \frac{yd}{z} = \frac{y\cot(\alpha / 2)}{z} = \frac{y}{z\tan(\alpha) / 2}
$$
*同时不难看出，若点(x, y, z)位于平截头体内，当且仅当：*
$$
-r \le x' \le r
\\\ \\\
-1 \le y' \le 1
\\\ \\\
n \le z \le f
$$

#### ***规格化设备坐标***

*由于硬件会涉及一些与投影窗口大小有关的操作，我们要去除投影窗口对纵横比的依赖，对此，我们的解决办法是将 x 坐标上的投影区间从[−r, r]缩放至归一化区间[−1, 1]， 就像下面这样：*
$$
-r \le x' \le r
\\\ \\\
-1 \le \frac{x'}{r} \le 1
$$
*经此映射处理后，x 坐标和 y 坐标就成为了规格化设备坐标*

*我们可以把由观察空间到 NDC 空间的变换视为一种单位换算*
$$
x' = \frac{x}{rz\tan(\alpha/2)}
\\\ \\\
y' = \frac{y}{z\tan{\alpha/2}}
$$
*注意，在 NDC 坐标中，投影窗口的高和宽都为 2，所以它的大小是固定的，硬件也就无须知道纵横比。但是，我们一定要确保将投影坐标映射到 NDC 空间内（图形硬件假设我们会完成这项工作）。*

#### ***用矩阵来表示投影公式***

$$
P = 
\begin{bmatrix}
\frac{1}{r\tan(\frac{\alpha}{2})} & 0 & 0 & 0
\\\ \\\
0 & \frac{1}{\tan(\frac{\alpha}{2})} & 0 & 0
\\\ \\\
0 & 0 & \frac{f}{f - n} & 1
\\\ \\\
0 & 0 & \frac{-nf}{f - n} & 0
\end{bmatrix}
$$

*在顶点乘以投影矩阵之后但还未进行透视除法之前，几何体会处于所谓的齐次裁剪空间（homogeneous clip space）或投影空间（projection space）之中。待完成透视除法之后，便是用规格化设备坐标（NDC）来表示几何体了。*

#### ***归一化深度值***

*待投影操作完毕后，所有的投影点都会位于 2D 投影窗口上，从而构成视觉上可见的 2D 图像。看起来，我 们似乎在此时就可以丢弃原始的 3D z 坐标了。然而，为了实现深度缓冲算法，我们仍需保留这些 3D 深度信息。 就像 Direct3D 希望将 x、y 坐标映射到归一化范围一样，深度坐标也要被映射到归一化区间[0, 1]以内。*

#### ***XMMatrixPerspectiveFovLH 函数***

*我们可以利用 DirectXMath 库内的 `XMMatrixPerspectiveFovLH ` 函数来构建透视投影矩阵：*

```cpp
// 返回投影矩阵
XMMATRIX XM_CALLCONV XMMatrixPerspectiveFovLH(
    float FovAngleY, // 用弧度制表示的垂直视场角
    float Aspect, // 纵横比 = 宽度 / 高度
    float NearZ, // 到近平面的距离
    float FarZ // 到远平面的距离
); 
```

*下面的代码片段详细解释了 XMMatrixPerspectiveFovLH 函数的用法。在此例中，我们将垂直视场角指定为 45° ，近平面位于 z = 1 处，远平面位于 z = 1000 处（这些长度皆以观察空间中的单位表示）。*

```cpp
XMMATRIX P = XMMatrixPerspectiveFovLH(0.25f*XM_PI, AspectRatio(), 1.0f, 1000.0f);
// 纵横比采用的是我们窗口的宽高比：
float D3DApp::AspectRatio()const
{
    return static_cast<float>(mClientWidth) / mClientHeight;
} 
```



## ***曲面细分阶段***

*曲面细分阶段（tessellation stages）是利用镶嵌化处理技术对网格中的三角形进行细分（subdivide）， 以此来增加物体表面上的三角形数量。再将这些新增的三角形偏移到适当的位置，使网格表现出更加细腻的细节*

*使用曲面细分的优点有以下几方面*

1. *我们能借此实现一种细节层次（level-of-detail，LOD）机制，使离虚拟摄像机较近的三角形经镶嵌化处理得到更加丰富的细节，而对距摄像机较远的三角形不进行任何更改。通过这种方式， 即可只针对用户关注度高的部分网格增添三角形，从而提升其细节效果*
2. *我们在内存中仅维护简单的低模（low-poly，低精度模型，也有译作低面多边形、低面片等）网格（低 模网格是指三角形数量较少的网格，已逐渐形成一门独特画风的艺术制作手段），再根据需求为它动态地增添额外的三角形，以此节省内存资源*
3. *我们可以在处理动画和物理模拟之时采用简单的低模网格，而仅在渲染的过程中使用经镶嵌化 处理的高模（high-poly，与低模对应）网格*

## ***几何着色器阶段***

*几何着色器（geometry shader stage，GS）是一个可选渲染阶段*

*我们可以利用几何着 色器将输入的图元拓展为一个或多个其他图元，抑或根据某些条件而选择不输出任何图元。顶点着色器与之相比，则不能创建顶点：它只能接受输入的单个顶点，经处理后再将该顶点输出。几何着色器的常见拿手好戏是将一个点或一条线扩展为一个四边形*

## ***裁剪***

*完全位于视锥体（viewing frustum，用户在 3D 空间中的可视范围（形如平截头体）亦常被称为视锥 体，也有译作视平截头体、视体、视景体等）之外的几何体需要被丢弃，而处于平截头体交界以外的几 何体部分也一定要接受被裁剪（clip）的操作。因此，只有在平截头体之内的物体对象才会最终保留下来。*

## ***光栅化阶段***

*光栅化阶段（rasterization stage，RS，亦有将 rasterization 译作像素化或栅格化）的主要任务是为投影至屏幕上的 3D 三角形计算出对应的像素颜色。*

### ***视口变换***

*当裁剪操作完成之后，硬件会通过透视除法将物体从齐次裁剪空间变换为规格化设备坐标（NDC）。 一旦物体的顶点位于 NDC 空间内，构成 2D 图像的 2D 顶点 x、y 坐标就会被变换到后台缓冲区中称为视 口（viewport）的矩形里。待此变换完成后，这些 x、y 坐标都将以像素为单位表示。通常来讲，由于 z 坐标常在深度缓冲技术中用作深度值，因此视口变换是不会影响此值的。即便如此，我 们还是可以通过修改 D3D12_VIEWPORT 结构体中的 MinDepth 和 MaxDepth 值来做到这一点。届时， 我们只需保证 MinDepth 和 MaxDepth 的取值为 0～1 即可。*

### ***背面剔除***

**

### ***顶点属性插值***

## ***像素着色器阶段***

*我们编写的像素着色器（pixel shader，PS）是一种由 GPU 来执行的程序。它会针对每一个像素片段（pixel fragment，亦有译作片元）进行处理（即每处理一个像素就要执行一次像素着色器），并根据顶点的插值属性作为输入来计算出对应的像素颜色。像素着色器既可以直接返回一种单一的恒定颜色，也可以实现如逐像素光照（per-pixel lighting）、反射（reflection）以及阴影（shadow）等更为复杂的效果*

## ***输出合并阶段***

*通过像素着色器生成的像素片段会被移送至渲染流水线的输出合并（Output Merger，OM）阶段。 在此阶段中，一些像素片段可能会被丢弃（例如，那些未通过深度缓冲区测试或模板缓冲区测试的像素 片段）。而后，剩下的像素片段将会被写入后台缓冲区中。混合（blend，也有译作融合）操作也是在此阶段实现的，此技术可令当前处理的像素与后台缓冲区中的对应像素相融合，而不仅是对后者进行完全 的覆写。一些如“透明”这样的特殊效果，也是由混合技术来实现的*

## ***小结***

1. *根据人们在真实生活中观察物体的经验，我们可以总结出一些规律。运用这些规律，我们便可 以通过 2D 图像模拟出 3D 效果的场景。我们可以观察到的规律有：平行线会聚于消失点，物体 的尺寸受其深度的影响（近大远小），离观察者近的物体会遮挡其后距观察者远的物体，光照与 阴影的明暗对比可刻画出 3D 物体的实体形状和体积感，阴影还暗示了光源的位置，并反映出场 景中不同物体之间的相对位置。*
2. *我们用三角形网格来近似地表示物体。并通过指定三角形的 3 个顶点来定义三角形。在许多网 格中都存在着顶点被不同三角形所共用的现象，而索引列表则可以用于避免因重复使用顶点而 复制顶点数据所带来的冗余信息*
3. *我们可以通过指定红、绿、蓝三色光的强度来描述颜色。利用此三色光不同强度的相加混色（additive mixing，也称加色法），可以使我们表示出数以千万计的颜色。我们通常用归一化范围 0～1 来描述 三色的强度，0 表示没有强度，1 表示最高强度，两者之间的值表示相应的中间强度。一般来说还会 加入另一种名为 alpha 分量（alpha component）的颜色分量。alpha 分量通常用于表示颜色的不透明 度，这在混合技术中是很有用的。算上 alpha 分量，我们就能用 4D 颜色向量(r, g, b, a)来表示颜色*
4. *给出某个 3D 场景的几何描述，并在此场景中设置一台具有特定位置与朝向的虚拟摄像机，那么 渲染流水线（rendering pipeline）就是根据该虚拟摄像机的视角，生成能呈现在显示器中对应 2D 图像的这一系列完整步骤*
5. *渲染流水线可以划分为输入装配（Input Assembly，IA）阶段、顶点着色器（Vertex Shader，VS） 阶段、曲面细分（tessellation）阶段、几何着色器（Geometry Shader，GS）阶段、裁剪阶段、光 栅化阶段（Rasterization Shage，RS）、像素着色器（Pixel Shader，PS）阶段以及输出合并（Output Merger，OM）等重要阶段*

